options:
  logging: CLOUD_LOGGING_ONLY
  
substitutions:
  _REGION: us-central1
  _REPO: services           
  _BUCKET: ${PROJECT_ID}-dataflow
  _SERVICE_ACCOUNT: df-domains@${PROJECT_ID}.iam.gserviceaccount.com
  
steps:
# Enable required APIs
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      gcloud services enable dataflow.googleapis.com
      gcloud services enable artifactregistry.googleapis.com
      
# Create bucket if it doesn't exist
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      gsutil ls gs://${_BUCKET} || gsutil mb -l ${_REGION} gs://${_BUCKET}
      
# Create Artifact Registry repo if it doesn't exist
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      gcloud artifacts repositories describe ${_REPO} --location=${_REGION} || \
      gcloud artifacts repositories create ${_REPO} --repository-format=docker --location=${_REGION}

# Build the Dataflow Flex Template image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'build'
    - '-f'
    - 'Dockerfile'
    - '-t'
    - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO}/supply-chain-pipeline:latest'
    - '.'
  
# Push the image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'push'
    - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO}/supply-chain-pipeline:latest'

# Build the Flex Template
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      gcloud dataflow flex-template build gs://${_BUCKET}/templates/supply-chain-pipeline.json \
        --image="${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO}/supply-chain-pipeline:latest" \
        --sdk-language=PYTHON \
        --metadata-file=metadata.json

# Test the pipeline with a simple job
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      # Get Redis host (assuming it's already created)
      REDIS_HOST=$(gcloud redis instances describe sc-idcache --region=${_REGION} --format='value(host)' 2>/dev/null || echo "10.0.0.1")
      
      # Get AlloyDB host (assuming it's already created) 
      ADB_HOST=$(gcloud alloydb instances describe sc-primary --cluster=sc-cluster --region=${_REGION} --format='value(ipAddress)' 2>/dev/null || echo "10.0.0.2")
      
      # Create a test job name with timestamp
      JOB_NAME="supply-chain-test-$(date +%Y%m%d-%H%M%S)"
      
      echo "Launching Dataflow job: $${JOB_NAME}"
      
      # Launch the Dataflow job
      gcloud dataflow flex-template run $${JOB_NAME} \
        --template-file-gcs-location=gs://${_BUCKET}/templates/supply-chain-pipeline.json \
        --region=${_REGION} \
        --service-account-email=${_SERVICE_ACCOUNT} \
        --parameters="project=${PROJECT_ID}" \
        --parameters="region=${_REGION}" \
        --parameters="supplier_sub=supplier.sub" \
        --parameters="material_sub=material.sub" \
        --parameters="plant_sub=plant.sub" \
        --parameters="redis_host=$${REDIS_HOST}" \
        --parameters="id_service_url=https://id-service-dummy.com/id" \
        --parameters="bq_supplier_bronze=${PROJECT_ID}:domain_bronze.supplier_bronze" \
        --parameters="bq_material_bronze=${PROJECT_ID}:domain_bronze.material_bronze" \
        --parameters="bq_plant_bronze=${PROJECT_ID}:domain_bronze.plant_bronze" \
        --parameters="bq_supplier_dlq=${PROJECT_ID}:domain_bronze.supplier_dlq" \
        --parameters="bq_material_dlq=${PROJECT_ID}:domain_bronze.material_dlq" \
        --parameters="bq_plant_dlq=${PROJECT_ID}:domain_bronze.plant_dlq" \
        --parameters="gcs_temp=gs://${_BUCKET}/temp" \
        --parameters="adb_host=$${ADB_HOST}" \
        --parameters="adb_port=5432" \
        --parameters="adb_user=postgres" \
        --parameters="adb_password=Str0ngP@ss!" \
        --parameters="adb_db=supplychain" \
        --max-workers=3 \
        --num-workers=1
      
      echo "Job submitted successfully: $${JOB_NAME}"
      
      # Wait a bit and check job status
      sleep 30
      gcloud dataflow jobs list --region=${_REGION} --limit=5 --format="table(name,state,createTime)"

timeout: 1800s
