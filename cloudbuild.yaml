# Minimal test cloudbuild.yaml - tests basic Dataflow functionality
options:
  logging: CLOUD_LOGGING_ONLY
  
substitutions:
  _REGION: us-central1
  _REPO: services           
  _BUCKET: ${PROJECT_ID}-dataflow
  
steps:
# Create a simple test main.py that just reads and writes
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      cat > test_main.py << 'EOF'
      import apache_beam as beam
      from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
      from apache_beam.io.gcp.bigquery import WriteToBigQuery
      import json
      import logging
      from datetime import datetime, timezone

      def parse_element(element):
          try:
              data = json.loads(element.decode('utf-8'))
              data['processed_at'] = datetime.now(timezone.utc).isoformat()
              return data
          except:
              return {"error": "parse_failed", "raw_data": str(element)}

      def run():
          import argparse
          parser = argparse.ArgumentParser()
          parser.add_argument("--output_table", required=True)
          parser.add_argument("--temp_location", required=True)
          args, beam_args = parser.parse_known_args()
          
          pipeline_options = PipelineOptions(beam_args)
          pipeline_options.view_as(StandardOptions).streaming = False
          
          with beam.Pipeline(options=pipeline_options) as p:
              result = (p 
                       | "Create" >> beam.Create([
                           '{"test": "data1", "id": 1}',
                           '{"test": "data2", "id": 2}'
                       ])
                       | "Parse" >> beam.Map(parse_element)
                       | "Write" >> WriteToBigQuery(
                           table=args.output_table,
                           custom_gcs_temp_location=args.temp_location,
                           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
                       ))

      if __name__ == "__main__":
          logging.getLogger().setLevel(logging.INFO)
          run()
      EOF

# Create simple requirements.txt
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      cat > test_requirements.txt << 'EOF'
      apache-beam[gcp]==2.53.0
      EOF

# Create simple Dockerfile
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      cat > test_Dockerfile << 'EOF'
      FROM gcr.io/dataflow-templates-base/python311-template-launcher-base
      ENV FLEX_TEMPLATE_PYTHON_PY_FILE="test_main.py"
      ENV FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE="test_requirements.txt"
      COPY test_requirements.txt /tmp/requirements.txt
      RUN pip install -r /tmp/requirements.txt
      COPY test_main.py /template/
      WORKDIR /template
      EOF

# Build test image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'build'
    - '-f'
    - 'test_Dockerfile'
    - '-t'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/test-pipeline:latest'
    - '.'

# Push test image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'push'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/test-pipeline:latest'

# Build test template
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      cat > test_metadata.json << 'EOF'
      {
        "name": "Test Pipeline",
        "description": "Simple test pipeline"
      }
      EOF
      
      gcloud dataflow flex-template build gs://${_BUCKET}/templates/test-pipeline.json \
        --image=us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/test-pipeline:latest \
        --sdk-language=PYTHON \
        --metadata-file=test_metadata.json

# Run test pipeline
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      JOB_NAME="test-pipeline-$(date +%Y%m%d-%H%M%S)"
      
      echo "Running test pipeline: $$JOB_NAME"
      
      gcloud dataflow flex-template run $$JOB_NAME \
        --region=${_REGION} \
        --template-file-gcs-location=gs://${_BUCKET}/templates/test-pipeline.json \
        --parameters="output_table=${PROJECT_ID}:domain_bronze.test_output,temp_location=gs://${_BUCKET}/temp"
      
      echo "Test job submitted: $$JOB_NAME"
      sleep 30
      
      gcloud dataflow jobs list --region=${_REGION} --format="table(name,state,createTime)" | head -3

timeout: 1200s
