# cloudbuild.yaml for sc-pipeline - FIXED WITH REAL VALUES
options:
  logging: CLOUD_LOGGING_ONLY
  substitution_option: ALLOW_LOOSE
  
substitutions:
  _REGION: us-central1
  _REPO: services           
  _BUCKET: ${PROJECT_ID}-dataflow  # FIXED: was *BUCKET
  # YOUR ACTUAL SERVICE VALUES:
  _REDIS_HOST: "10.185.247.59"  # Your Redis HOST IP
  _ID_SERVICE_URL: "https://id-service-1051481682783.us-central1.run.app"  # Your Cloud Run service URL
  _ADB_HOST: "10.100.129.2"  # AlloyDB private IP
  
steps:
# 0) Make sure the Dataflow bucket exists
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args: 
    - '-c'
    - |
      echo "Checking if bucket gs://${_BUCKET} exists..."
      if ! gsutil ls gs://${_BUCKET} >/dev/null 2>&1; then
        echo "Creating bucket gs://${_BUCKET}..."
        gsutil mb -l ${_REGION} gs://${_BUCKET}
      else
        echo "Bucket already exists"
      fi

# 1) Build image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'build'
    - '-t'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest'
    - '-t'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:${SHORT_SHA}'
    - '.'

# 2) Push image
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'push'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest'
- name: gcr.io/cloud-builders/docker:latest
  args:
    - 'push'
    - 'us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:${SHORT_SHA}'

# 3) Build the Dataflow Flex Template spec in GCS
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      echo "Building Dataflow Flex Template..."
      gcloud dataflow flex-template build gs://${_BUCKET}/templates/sc-pipeline.json \
        --image=us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest \
        --sdk-language=PYTHON \
        --metadata-file=metadata.json \
        --project=${PROJECT_ID}

# 4) Verify template creation
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      echo "Verifying template was created..."
      gsutil ls -la gs://${_BUCKET}/templates/sc-pipeline.json

# 5) Run the pipeline with YOUR REAL VALUES
- name: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
  entrypoint: bash
  args:
    - '-c'
    - |
      echo "Starting Dataflow pipeline..."
      
      JOB_NAME="sc-pipeline-$(date +%Y%m%d-%H%M%S)"
      
      gcloud dataflow flex-template run $${JOB_NAME} \
        --region=us-central1 \
        --template-file-gcs-location=gs://${_BUCKET}/templates/sc-pipeline.json \
        --network=default \
        --subnetwork=regions/us-central1/subnetworks/default \
        --max-workers=10 \
        --parameters="project=${PROJECT_ID},region=us-central1,supplier_sub=supplier.sub,material_sub=material.sub,plant_sub=plant.sub,redis_host=${_REDIS_HOST},id_service_url=${_ID_SERVICE_URL}/id,bq_supplier_bronze=${PROJECT_ID}:domain_bronze.supplier_bronze,bq_material_bronze=${PROJECT_ID}:domain_bronze.material_bronze,bq_plant_bronze=${PROJECT_ID}:domain_bronze.plant_bronze,bq_supplier_dlq=${PROJECT_ID}:domain_bronze.supplier_dlq,bq_material_dlq=${PROJECT_ID}:domain_bronze.material_dlq,bq_plant_dlq=${PROJECT_ID}:domain_bronze.plant_dlq,gcs_temp=gs://${_BUCKET}/bqtmp,adb_host=${_ADB_HOST},adb_port=5432,adb_user=postgres,adb_password=Str0ngP@ss!,adb_db=supplychain"
      
      echo "Pipeline $${JOB_NAME} started successfully!"

timeout: 1800s
