# cloudbuild.yaml for sc-pipeline
options:
  logging: CLOUD_LOGGING_ONLY

substitutions:
  _REGION: us-central1
  _REPO: services           
  _BUCKET: rare-result-471319-e6-dataflow   

steps:
# 0) Make sure the Dataflow bucket exists
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  entrypoint: bash
  args: ['-lc', 'gsutil ls gs://${_BUCKET} || gsutil mb -l ${_REGION} gs://${_BUCKET}']

# 1) Build image
- name: gcr.io/cloud-builders/docker
  args:
    ['build',
     '-t','us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest',
     '.']

# 2) Push image
- name: gcr.io/cloud-builders/docker
  args:
    ['push','us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest']

# 3) Build the Dataflow Flex Template spec in GCS
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  args:
    ['dataflow','flex-template','build','gs://${_BUCKET}/templates/sc-pipeline.json',
     '--image','us-central1-docker.pkg.dev/${PROJECT_ID}/${_REPO}/sc-pipeline:latest',
     '--sdk-language','PYTHON',
     '--metadata-file','metadata.json']

